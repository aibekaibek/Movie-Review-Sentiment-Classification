{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9940d6",
   "metadata": {},
   "source": [
    "## Цель этапа обучения\n",
    "\n",
    "Цель данного этапа — обучить базовую LSTM-модель\n",
    "для бинарной классификации отзывов на положительные и отрицательные.\n",
    "\n",
    "Данная модель используется как baseline (точка отсчёта),\n",
    "с которой в дальнейшем будут сравниваться улучшенные архитектуры.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e89b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8870e",
   "metadata": {},
   "source": [
    "## Загрузка данных и словаря\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1351ced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurs\\AppData\\Local\\Temp\\ipykernel_1864\\1425761307.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(ARTIFACTS_DIR / \"dataset.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([35000, 400]), torch.Size([35000]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "data = torch.load(ARTIFACTS_DIR / \"dataset.pt\")\n",
    "X_train = data[\"X_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "X_val = data[\"X_val\"]\n",
    "y_val = data[\"y_val\"]\n",
    "X_test = data[\"X_test\"]\n",
    "y_test = data[\"y_test\"]\n",
    "with open(ARTIFACTS_DIR / \"word2idx.pkl\", \"rb\") as f:\n",
    "    word2idx = pickle.load(f)\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a3040",
   "metadata": {},
   "source": [
    "На данном этапе используется уже подготовленный датасет,\n",
    "сохранённый на этапе Data Preprocessing.\n",
    "Повторная обработка текста не выполняется.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4aab83",
   "metadata": {},
   "source": [
    "## Подготовка Dataset и DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff32aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBTensorDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5efedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = IMDBTensorDataset(X_train, y_train)\n",
    "val_dataset   = IMDBTensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05044a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 400]), torch.Size([64]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_X, batch_y = next(iter(train_loader))\n",
    "batch_X.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf197af",
   "metadata": {},
   "source": [
    "DataLoader обеспечивает:\n",
    "- обучение батчами\n",
    "- перемешивание данных на этапе обучения\n",
    "- корректную подачу данных в модель\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13582033",
   "metadata": {},
   "source": [
    "## Архитектура базовой LSTM-модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fe8eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        padding_idx\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Embedding слой\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        # 2) LSTM слой\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # 3) Выходной слой\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        # (batch, seq) → (batch, seq, embed)\n",
    "        embedded = self.embedding(x)\n",
    "        # LSTM\n",
    "        # hidden: (num_layers, batch, hidden_dim)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        # берём последнее скрытое состояние\n",
    "        hidden = hidden.squeeze(0)\n",
    "        # (batch, hidden_dim) → (batch, 1)\n",
    "        logits = self.fc(hidden)\n",
    "        return logits.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaddf384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(20000, 128, padding_idx=0)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX = 0\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    padding_idx=PAD_IDX\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750a93b",
   "metadata": {},
   "source": [
    "Модель состоит из:\n",
    "- embedding слоя\n",
    "- одного LSTM слоя\n",
    "- линейного классификатора\n",
    "\n",
    "Выход модели — логиты, используемые для бинарной классификации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6c480fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(batch_X)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed31a2",
   "metadata": {},
   "source": [
    "## Функция потерь и оптимизатор\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088a032a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.691602885723114"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "batch_X, batch_y = next(iter(train_loader))\n",
    "batch_X = batch_X.to(device)\n",
    "batch_y = batch_y.to(device)\n",
    "optimizer.zero_grad()\n",
    "logits = model(batch_X)\n",
    "loss = criterion(logits, batch_y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73208b15",
   "metadata": {},
   "source": [
    "Используется функция потерь BCEWithLogitsLoss,\n",
    "которая является стандартом для бинарной классификации\n",
    "и численно более стабильна, чем sigmoid + BCELoss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b651fea",
   "metadata": {},
   "source": [
    "## Цикл обучения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ff813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b6eb922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a7e6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.6930 | Val Loss: 0.6927\n",
      "Epoch 02 | Train Loss: 0.6844 | Val Loss: 0.6932\n",
      "Epoch 03 | Train Loss: 0.6663 | Val Loss: 0.6782\n",
      "Epoch 04 | Train Loss: 0.6560 | Val Loss: 0.7050\n",
      "Epoch 05 | Train Loss: 0.6431 | Val Loss: 0.7117\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "    val_loss = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63761736",
   "metadata": {},
   "source": [
    "В процессе обучения наблюдается снижение training loss,\n",
    "однако validation loss начинает расти после нескольких эпох,\n",
    "что указывает на переобучение базовой модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1f035",
   "metadata": {},
   "source": [
    "## Оценка качества модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c23ec76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5045\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = compute_accuracy(model, val_loader, device)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7cbf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5068\n"
     ]
    }
   ],
   "source": [
    "test_dataset = IMDBTensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "test_accuracy = compute_accuracy(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dfd246",
   "metadata": {},
   "source": [
    "Accuracy используется как основная метрика,\n",
    "поскольку датасет IMDb является сбалансированным.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e107808",
   "metadata": {},
   "source": [
    "## Сохранение базовой модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d30a7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    MODEL_DIR / \"baseline_lstm.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893c8e5",
   "metadata": {},
   "source": [
    "## Итог\n",
    "\n",
    "Была обучена базовая LSTM-модель для задачи анализа тональности отзывов IMDb.\n",
    "\n",
    "Модель демонстрирует обучение на тренировочных данных,\n",
    "однако её качество на validation и test выборках\n",
    "близко к случайному, что указывает на ограниченность архитектуры.\n",
    "\n",
    "Данная модель используется как baseline.\n",
    "Следующим этапом является улучшение архитектуры\n",
    "с использованием BiLSTM.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
