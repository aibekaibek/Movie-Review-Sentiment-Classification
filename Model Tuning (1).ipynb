{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcaa8908",
   "metadata": {},
   "source": [
    "# Улучшения моделя\n",
    "\n",
    "Цель данного этапа — улучшить качество базовой модели классификации\n",
    "тональности текстовых отзывов IMDb за счёт архитектурных улучшений\n",
    "и подбора гиперпараметров.\n",
    "\n",
    "На данном этапе данные и процедура предобработки считаются фиксированными.\n",
    "Все изменения касаются исключительно архитектуры модели и её параметров.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8bf50c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nurs\\PycharmProjects\\pythonProject5\\nlp_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4b5e5",
   "metadata": {},
   "source": [
    "###  Загрузка подготовленных данных и артефактов\n",
    "\n",
    "На данном этапе мы загружаем результаты предыдущих шагов:\n",
    "- подготовленные тензоры train / validation / test\n",
    "- словарь word2idx\n",
    "- фиксируем размер словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87050fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "MODELS_DIR = Path(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db79383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurs\\AppData\\Local\\Temp\\ipykernel_18456\\2661674695.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(ARTIFACTS_DIR / \"dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(ARTIFACTS_DIR / \"dataset.pt\")\n",
    "X_train = data[\"X_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "X_val = data[\"X_val\"]\n",
    "y_val = data[\"y_val\"]\n",
    "X_test = data[\"X_test\"]\n",
    "y_test = data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c40e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ARTIFACTS_DIR / \"word2idx.pkl\", \"rb\") as f:\n",
    "    word2idx = pickle.load(f)\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "PAD_IDX = word2idx[\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e3ef8",
   "metadata": {},
   "source": [
    "### Создание Dataset и DataLoader\n",
    "\n",
    "Для обучения и оценки модели используются PyTorch Dataset и DataLoader.\n",
    "Это позволяет:\n",
    "- эффективно загружать данные батчами\n",
    "- использовать перемешивание для train выборки\n",
    "- обеспечить единый интерфейс для обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd0471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBTensorDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd7c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = IMDBTensorDataset(X_train, y_train)\n",
    "val_dataset   = IMDBTensorDataset(X_val, y_val)\n",
    "test_dataset  = IMDBTensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa226c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebbd43cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 400]), torch.Size([64]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_X, batch_y = next(iter(train_loader))\n",
    "batch_X.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01919d7",
   "metadata": {},
   "source": [
    "## Наша базовая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b1f8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        logits = self.fc(hidden)\n",
    "        return logits.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f989644e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurs\\AppData\\Local\\Temp\\ipykernel_18456\\659086518.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(MODELS_DIR / \"baseline_lstm.pt\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(20000, 128, padding_idx=0)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "baseline_model = LSTMClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    padding_idx=PAD_IDX\n",
    ")\n",
    "baseline_model.load_state_dict(\n",
    "    torch.load(MODELS_DIR / \"baseline_lstm.pt\", map_location=device)\n",
    ")\n",
    "baseline_model = baseline_model.to(device)\n",
    "baseline_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f2dd19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = baseline_model(batch_X.to(device))\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "551b5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cc90e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5045333333333333, 0.5068)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracy_baseline = compute_accuracy(baseline_model, val_loader, device)\n",
    "test_accuracy_baseline = compute_accuracy(baseline_model, test_loader, device)\n",
    "val_accuracy_baseline, test_accuracy_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5231a",
   "metadata": {},
   "source": [
    "###  Архитектура модели с Masked Pooling и Dropout\n",
    "\n",
    "В отличие от базовой BiLSTM, здесь используется:\n",
    "- masked mean pooling\n",
    "- masked max pooling\n",
    "- dropout для регуляризации\n",
    "\n",
    "Это позволяет учитывать информацию по всей последовательности\n",
    "и снижает влияние padding-токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adbe8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        padding_idx\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        forward_hidden = hidden[0]\n",
    "        backward_hidden = hidden[1]\n",
    "        combined = torch.cat(\n",
    "            (forward_hidden, backward_hidden),\n",
    "            dim=1\n",
    "        )\n",
    "        logits = self.fc(combined)\n",
    "        return logits.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd116e",
   "metadata": {},
   "source": [
    "###  Обучение модели и выбор лучшего чекпойнта\n",
    "\n",
    "Модель обучается в течение нескольких эпох.\n",
    "Лучший чекпойнт выбирается на основе минимального значения validation loss,\n",
    "что позволяет избежать переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42427f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMClassifier(\n",
       "  (embedding): Embedding(20000, 128, padding_idx=0)\n",
       "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTMClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    padding_idx=PAD_IDX\n",
    ")\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36c1414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.2343 | Val Loss: 0.3959 | Val Acc: 0.8492\n",
      "Epoch 02 | Train Loss: 0.1791 | Val Loss: 0.4031 | Val Acc: 0.8563\n",
      "Epoch 03 | Train Loss: 0.1865 | Val Loss: 0.4901 | Val Acc: 0.8127\n",
      "Epoch 04 | Train Loss: 0.1396 | Val Loss: 0.4272 | Val Acc: 0.8643\n",
      "Epoch 05 | Train Loss: 0.1083 | Val Loss: 0.4373 | Val Acc: 0.8529\n",
      "\n",
      "Best epoch: 1 | Best val loss: 0.3959\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "EPOCHS = 5\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "best_epoch = None\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "\n",
    "    val_loss = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    val_acc = compute_accuracy(\n",
    "        model, val_loader, device\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {\n",
    "            k: v.detach().cpu().clone()\n",
    "            for k, v in model.state_dict().items()\n",
    "        }\n",
    "        best_epoch = epoch\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "print(f\"\\nBest epoch: {best_epoch} | Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c19b3f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMClassifier(\n",
       "  (embedding): Embedding(20000, 128, padding_idx=0)\n",
       "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_state)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3012a09",
   "metadata": {},
   "source": [
    "### Шаг 5. Оценка качества модели на test выборке\n",
    "\n",
    "После обучения загружается лучший чекпойнт,\n",
    "и вычисляется итоговая accuracy на test данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dce5c8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (BiLSTM, best checkpoint): 0.8472\n"
     ]
    }
   ],
   "source": [
    "test_accuracy_best = compute_accuracy(\n",
    "    model,\n",
    "    test_loader,\n",
    "    device\n",
    ")\n",
    "print(f\"Test Accuracy (BiLSTM, best checkpoint): {test_accuracy_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e7d2a",
   "metadata": {},
   "source": [
    "## BiLSTM с Masked Pooling и Dropout \n",
    "Основные идеи данного шага:\n",
    "- учитывать вклад каждого слова в отзыве, а не только крайние токены\n",
    "- полностью игнорировать padding-токены при агрегации признаков\n",
    "- снизить переобучение с помощью dropout\n",
    "\n",
    "Для этого используются masked mean pooling и masked max pooling,\n",
    "которые позволяют сформировать устойчивое и информативное\n",
    "представление всего текста. BiLSTM с Masked Pooling и Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b514732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMPoolingClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)              \n",
    "        output, _ = self.lstm(embedded)         \n",
    "        mask = (x != self.padding_idx)              \n",
    "        mask = mask.unsqueeze(-1)                    \n",
    "        output_masked = output * mask           \n",
    "        lengths = mask.sum(dim=1).clamp(min=1)       \n",
    "        mean_pool = output_masked.sum(dim=1) / lengths  \n",
    "        output_for_max = output.masked_fill(~mask, -1e9)\n",
    "        max_pool = output_for_max.max(dim=1).values  \n",
    "        pooled = torch.cat([mean_pool, max_pool], dim=1)  \n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.fc(pooled).squeeze(1)        \n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5532df10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POOL_DROPOUT = 0.5\n",
    "\n",
    "model_pool = BiLSTMPoolingClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    padding_idx=PAD_IDX,\n",
    "    dropout=POOL_DROPOUT\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model_pool(batch_X.to(device))\n",
    "\n",
    "logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b83ebeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.4575 | Val Loss: 0.3208 | Val Acc: 0.8637\n",
      "Epoch 02 | Train Loss: 0.2609 | Val Loss: 0.2684 | Val Acc: 0.8917\n",
      "Epoch 03 | Train Loss: 0.2051 | Val Loss: 0.2699 | Val Acc: 0.8884\n",
      "Epoch 04 | Train Loss: 0.1487 | Val Loss: 0.2805 | Val Acc: 0.8933\n",
      "Epoch 05 | Train Loss: 0.1012 | Val Loss: 0.3011 | Val Acc: 0.8963\n",
      "\n",
      "Best epoch: 2 | Best val loss: 0.2684\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_pool.parameters(), lr=1e-3)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "best_epoch = None\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model_pool, train_loader, optimizer, criterion, device)\n",
    "    val_loss   = evaluate(model_pool, val_loader, criterion, device)\n",
    "    val_acc    = compute_accuracy(model_pool, val_loader, device)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model_pool.state_dict().items()}\n",
    "        best_epoch = epoch\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nBest epoch: {best_epoch} | Best val loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f34d7f",
   "metadata": {},
   "source": [
    "## Оценка модели BiLSTM с Masked Pooling и Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4d7274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (BiLSTM + Pooling + Dropout, best): 0.9015\n"
     ]
    }
   ],
   "source": [
    "model_pool.load_state_dict(best_state)\n",
    "model_pool.to(device)\n",
    "test_acc = compute_accuracy(model_pool, test_loader, device)\n",
    "print(f\"Test Accuracy (BiLSTM + Pooling + Dropout, best): {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d58516d",
   "metadata": {},
   "source": [
    "В результате применения данной архитектуры была достигнута\n",
    "accuracy около 0.90 на test выборке,\n",
    "что значительно превосходит результаты базовой BiLSTM модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23edfcb",
   "metadata": {},
   "source": [
    "## Подбор гиперпараметров (Optuna)\n",
    "\n",
    "После архитектурных улучшений следующим этапом является\n",
    "подбор гиперпараметров модели.\n",
    "\n",
    "Цель данного шага — найти оптимальную конфигурацию модели,\n",
    "которая обеспечивает наилучшее качество на validation выборке.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c848b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.6)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
    "\n",
    "    model = BiLSTMPoolingClassifier(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=hidden_dim,\n",
    "        padding_idx=PAD_IDX,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    EPOCHS = 3 \n",
    "\n",
    "    for _ in range(EPOCHS):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "    val_acc = compute_accuracy(model, val_loader, device)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "988a49bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-16 17:57:59,352] A new study created in memory with name: no-name-38c30e6e-aa12-4369-acb0-ff228df5e133\n",
      "[I 2025-12-16 17:58:21,145] Trial 0 finished with value: 0.8332 and parameters: {'hidden_dim': 64, 'dropout': 0.5679973463480082, 'lr': 0.0002835043270899687}. Best is trial 0 with value: 0.8332.\n",
      "[I 2025-12-16 17:58:42,181] Trial 1 finished with value: 0.7876 and parameters: {'hidden_dim': 64, 'dropout': 0.377379684563925, 'lr': 0.00012118126112518238}. Best is trial 0 with value: 0.8332.\n",
      "[I 2025-12-16 18:01:16,947] Trial 2 finished with value: 0.8569333333333333 and parameters: {'hidden_dim': 128, 'dropout': 0.23135716278985619, 'lr': 0.0002450578989315937}. Best is trial 2 with value: 0.8569333333333333.\n",
      "[I 2025-12-16 18:03:31,141] Trial 3 finished with value: 0.8970666666666667 and parameters: {'hidden_dim': 64, 'dropout': 0.4576022212944503, 'lr': 0.002167795446235008}. Best is trial 3 with value: 0.8970666666666667.\n",
      "[I 2025-12-16 18:14:03,217] Trial 4 finished with value: 0.9077333333333333 and parameters: {'hidden_dim': 256, 'dropout': 0.41375295777293747, 'lr': 0.0012688497338454597}. Best is trial 4 with value: 0.9077333333333333.\n",
      "[I 2025-12-16 18:17:31,071] Trial 5 finished with value: 0.8488 and parameters: {'hidden_dim': 128, 'dropout': 0.5735055265645455, 'lr': 0.00023220422555958773}. Best is trial 4 with value: 0.9077333333333333.\n",
      "[I 2025-12-16 18:28:03,537] Trial 6 finished with value: 0.8870666666666667 and parameters: {'hidden_dim': 256, 'dropout': 0.24042650058965753, 'lr': 0.0004195786682256528}. Best is trial 4 with value: 0.9077333333333333.\n",
      "[I 2025-12-16 18:38:36,562] Trial 7 finished with value: 0.9001333333333333 and parameters: {'hidden_dim': 256, 'dropout': 0.3123360311118547, 'lr': 0.0019775204960537055}. Best is trial 4 with value: 0.9077333333333333.\n",
      "[I 2025-12-16 18:42:30,272] Trial 8 finished with value: 0.9009333333333334 and parameters: {'hidden_dim': 128, 'dropout': 0.3921387140248564, 'lr': 0.001359550743695614}. Best is trial 4 with value: 0.9077333333333333.\n",
      "[I 2025-12-16 18:46:27,093] Trial 9 finished with value: 0.8584 and parameters: {'hidden_dim': 128, 'dropout': 0.5458485788162177, 'lr': 0.0002647552793512187}. Best is trial 4 with value: 0.9077333333333333.\n",
      "[I 2025-12-16 18:57:10,133] Trial 10 finished with value: 0.8808 and parameters: {'hidden_dim': 256, 'dropout': 0.4730473428274851, 'lr': 0.0009454602720288174}. Best is trial 4 with value: 0.9077333333333333.\n",
      "[I 2025-12-16 19:01:07,768] Trial 11 finished with value: 0.8969333333333334 and parameters: {'hidden_dim': 128, 'dropout': 0.37144376883800806, 'lr': 0.0010489375704601574}. Best is trial 4 with value: 0.9077333333333333.\n",
      "[I 2025-12-16 19:11:52,908] Trial 12 finished with value: 0.8978666666666667 and parameters: {'hidden_dim': 256, 'dropout': 0.44321085945289257, 'lr': 0.0010804639990551026}. Best is trial 4 with value: 0.9077333333333333.\n",
      "[I 2025-12-16 19:15:51,674] Trial 13 finished with value: 0.8893333333333333 and parameters: {'hidden_dim': 128, 'dropout': 0.3125443472903828, 'lr': 0.002981274640772287}. Best is trial 4 with value: 0.9077333333333333.\n",
      "[I 2025-12-16 19:26:27,149] Trial 14 finished with value: 0.8916 and parameters: {'hidden_dim': 256, 'dropout': 0.5032899354650391, 'lr': 0.0007182483548776456}. Best is trial 4 with value: 0.9077333333333333.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e206741",
   "metadata": {},
   "source": [
    "### Результаты оптимизации Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de70c468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=4, state=<TrialState.COMPLETE: 1>, values=[0.9077333333333333], datetime_start=datetime.datetime(2025, 12, 16, 18, 3, 31, 142415), datetime_complete=datetime.datetime(2025, 12, 16, 18, 14, 3, 217092), params={'hidden_dim': 256, 'dropout': 0.41375295777293747, 'lr': 0.0012688497338454597}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'hidden_dim': CategoricalDistribution(choices=(64, 128, 256)), 'dropout': FloatDistribution(high=0.6, log=False, low=0.2, step=None), 'lr': FloatDistribution(high=0.003, log=True, low=0.0001, step=None)}, trial_id=4, value=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91c18677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.9077333333333333\n",
      "Best hyperparameters:\n",
      "  hidden_dim: 256\n",
      "  dropout: 0.41375295777293747\n",
      "  lr: 0.0012688497338454597\n"
     ]
    }
   ],
   "source": [
    "print(\"Best validation accuracy:\", study.best_value)\n",
    "print(\"Best hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7f698",
   "metadata": {},
   "source": [
    "В результате оптимизации был найден следующий набор\n",
    "гиперпараметров, обеспечивший наилучшее качество\n",
    "на validation выборке:\n",
    "\n",
    "- hidden_dim — размер скрытого состояния LSTM\n",
    "- dropout — коэффициент dropout для регуляризации\n",
    "- learning rate — скорость обучения оптимизатора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e0654",
   "metadata": {},
   "source": [
    "### Финальное обучение модели с оптимальными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8dcde01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.4229 | Val Loss: 0.2895\n",
      "Epoch 02 | Train Loss: 0.2371 | Val Loss: 0.2512\n",
      "Epoch 03 | Train Loss: 0.1663 | Val Loss: 0.2365\n",
      "Epoch 04 | Train Loss: 0.1109 | Val Loss: 0.2811\n",
      "Epoch 05 | Train Loss: 0.0652 | Val Loss: 0.2972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiLSTMPoolingClassifier(\n",
       "  (embedding): Embedding(20000, 128, padding_idx=0)\n",
       "  (lstm): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.41375295777293747, inplace=False)\n",
       "  (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "final_model = BiLSTMPoolingClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    padding_idx=PAD_IDX,\n",
    "    dropout=best_params[\"dropout\"]\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\n",
    "EPOCHS = 5\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(final_model, train_loader, optimizer, criterion, device)\n",
    "    val_loss   = evaluate(final_model, val_loader, criterion, device)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in final_model.state_dict().items()}\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "final_model.load_state_dict(best_state)\n",
    "final_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d53eaa5",
   "metadata": {},
   "source": [
    "Выше представлена финальная архитектура модели\n",
    "после применения архитектурных улучшений\n",
    "и подбора гиперпараметров.\n",
    "\n",
    "Модель включает:\n",
    "- embedding слой\n",
    "- двунаправленную LSTM с увеличенным hidden_dim\n",
    "- masked mean и max pooling\n",
    "- dropout для регуляризации\n",
    "- линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501703c2",
   "metadata": {},
   "source": [
    "### Финальная оценка модели на test выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63f934b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy (Optuna tuned): 0.9117\n"
     ]
    }
   ],
   "source": [
    "final_test_acc = compute_accuracy(final_model, test_loader, device)\n",
    "print(f\"Final Test Accuracy (Optuna tuned): {final_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181bfcd6",
   "metadata": {},
   "source": [
    "## Сохранение нашей улучшенной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a470eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    final_model.state_dict(),\n",
    "    \"artifacts/bilstm_pooling_optuna_best.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dde6bc",
   "metadata": {},
   "source": [
    "### Финальный вывод по этапу Model Tuning\n",
    "\n",
    "В ходе этапа Model Tuning были выполнены:\n",
    "- архитектурные улучшения модели\n",
    "- устранение влияния padding-токенов с помощью masked pooling\n",
    "- регуляризация с использованием dropout\n",
    "- автоматический подбор гиперпараметров с помощью Optuna\n",
    "\n",
    "Финальная модель достигла accuracy 0.91 на test выборке,\n",
    "что значительно превосходит результаты базовой модели\n",
    "и подтверждает эффективность выбранного подхода.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
